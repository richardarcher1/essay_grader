{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.059308Z",
     "start_time": "2024-12-13T18:09:30.187637Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import polars as pl"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.067080Z",
     "start_time": "2024-12-13T18:09:31.065040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        # embeddings: list of numpy arrays or torch tensors\n",
    "        # labels: list of scalars\n",
    "        self.X = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32)  # or long, depending on your task\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "id": "a1e337aa14cbd065",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.136386Z",
     "start_time": "2024-12-13T18:09:31.118386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset = torch.load(\"../data/2_ready_for_training/embedded/train_dataset_large.pth\")\n",
    "testing_dataset = torch.load(\"../data/2_ready_for_training/embedded/test_dataset_large.pth\")\n",
    "val_dataset = torch.load(\"../data/2_ready_for_training/embedded/val_dataset_large.pth\")"
   ],
   "id": "c23d895ea0026ad5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12873/3699622598.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training_dataset = torch.load(\"../data/2_ready_for_training/embedded/train_dataset_large.pth\")\n",
      "/tmp/ipykernel_12873/3699622598.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  testing_dataset = torch.load(\"../data/2_ready_for_training/embedded/test_dataset_large.pth\")\n",
      "/tmp/ipykernel_12873/3699622598.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_dataset = torch.load(\"../data/2_ready_for_training/embedded/val_dataset_large.pth\")\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.168326Z",
     "start_time": "2024-12-13T18:09:31.165637Z"
    }
   },
   "cell_type": "code",
   "source": "len(val_dataset), len(testing_dataset), len(training_dataset)",
   "id": "8716417ed8e7d090",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(622, 1952, 10403)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.214897Z",
     "start_time": "2024-12-13T18:09:31.212654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a simple two-layer neural net with dropout and layer norm\n",
    "class LargeEmbeddingNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LargeEmbeddingNeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.layer3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ],
   "id": "78573236c92a6089",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.259687Z",
     "start_time": "2024-12-13T18:09:31.258160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dim = int(testing_dataset.X[0].shape[0])\n",
    "hidden_dim = int(testing_dataset.X[0].shape[0]/2)"
   ],
   "id": "8c82e0cbcd4a9655",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.413909Z",
     "start_time": "2024-12-13T18:09:31.318271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LargeEmbeddingNeuralNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=1)\n",
    "model = model.to(\"cuda\")"
   ],
   "id": "caa2430e35069858",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.891525Z",
     "start_time": "2024-12-13T18:09:31.417558Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
   "id": "fe02efe7fc3bba6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.895797Z",
     "start_time": "2024-12-13T18:09:31.894433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# TODO  ADD IN LATER BUT NOT YET\n",
    "#\n",
    "# class FlatLRScheduler(_LRScheduler):\n",
    "#     def __init__(self, optimizer: Optimizer, schedule: list, last_epoch: int = -1):\n",
    "#         \"\"\"\n",
    "#         Custom Learning Rate Scheduler.\n",
    "#\n",
    "#         Args:\n",
    "#             optimizer (Optimizer): Wrapped optimizer.\n",
    "#             schedule (list): List of tuples defining (epoch_range, lr_value).\n",
    "#                              Example: [(9, 1e-3), (8, 1e-4), (8, 1e-5)]\n",
    "#             last_epoch (int): The index of the last epoch. Default is -1.\n",
    "#         \"\"\"\n",
    "#         self.schedule = []\n",
    "#         total_epochs = 0\n",
    "#         for epochs, lr in schedule:\n",
    "#             total_epochs += epochs\n",
    "#             self.schedule.append((total_epochs, lr))\n",
    "#         super().__init__(optimizer, last_epoch)\n",
    "#\n",
    "#     def get_lr(self):\n",
    "#         current_epoch = self.last_epoch + 1\n",
    "#         for end_epoch, lr in self.schedule:\n",
    "#             if current_epoch <= end_epoch:\n",
    "#                 return [lr for _ in self.optimizer.param_groups]\n",
    "#         raise ValueError(f\"Epoch {current_epoch} is out of range for the provided schedule.\")\n",
    "#\n",
    "# Define the schedule: [(9 epochs at 1e-3), (8 epochs at 1e-4), (8 epochs at 1e-5)]\n",
    "# schedule = [(9, 1e-3), (8, 1e-4), (8, 1e-5)]\n",
    "# schedule = [(10, 1e-3), (10, 1e-4), (5, 1e-5)]\n",
    "# schedule = [(15, 1e-3), (16, 1e-4)] # THIS SHOULD BE ONE LONGER THAN NUM_EPOCHS TO AVOID OFF BY ONE ERROR\n",
    "#\n",
    "# Initialize the custom scheduler\n",
    "# scheduler = FlatLRScheduler(optimizer, schedule)\n"
   ],
   "id": "ddba297de9d80fcf",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.937579Z",
     "start_time": "2024-12-13T18:09:31.936257Z"
    }
   },
   "cell_type": "code",
   "source": "criterion = nn.MSELoss()",
   "id": "eb623e60d13f6ef",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:31.979868Z",
     "start_time": "2024-12-13T18:09:31.978698Z"
    }
   },
   "cell_type": "code",
   "source": "num_epochs = 15",
   "id": "901e6b435c621151",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:32.023121Z",
     "start_time": "2024-12-13T18:09:32.021642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data loaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False)\n"
   ],
   "id": "9bd51edc1b2c8b1c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:09:32.295066Z",
     "start_time": "2024-12-13T18:09:32.066482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "lr_s = []\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # for inputs, labels in tqdm(test_loader, desc=f\"Testing Epoch {epoch_num}\"):\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(-1), labels)\n",
    "            # test_loss += loss.item()\n",
    "            test_loss += loss.item() * inputs.size(0)  # Multiply batch loss by batch size\n",
    "        avg_loss = test_loss/len(testing_dataset)\n",
    "        test_losses.append(avg_loss)\n",
    "        print(f\"TEST LOSS: PRE Epoch-{epoch_num}, we have {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch_num}\"):\n",
    "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # loss = criterion(outputs, labels) # MY MSITAKE - DO NOT DO THIS\n",
    "        loss = criterion(outputs.squeeze(-1), labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # train_loss += loss.item()\n",
    "        train_loss += loss.item() * inputs.size(0)  # Multiply batch loss by batch size\n",
    "    avg_loss = train_loss/len(training_dataset) # /len(train_dataset) ?? TODO\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"TRAIN LOSS: On Epoch-{epoch_num}, we have {avg_loss:.4f}\")\n",
    "\n",
    "    # scheduler.step()\n",
    "    # current_lr = scheduler.get_last_lr()\n",
    "    # lr_s.append(current_lr)\n",
    "\n",
    "\n"
   ],
   "id": "880f53ab870d9e8e",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m test_loader:\n\u001B[1;32m     12\u001B[0m     inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m), labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), labels)\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# test_loss += loss.item()\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[5], line 20\u001B[0m, in \u001B[0;36mLargeEmbeddingNeuralNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 20\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n\u001B[1;32m     22\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/normalization.py:217\u001B[0m, in \u001B[0;36mLayerNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 217\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/nn/functional.py:2900\u001B[0m, in \u001B[0;36mlayer_norm\u001B[0;34m(input, normalized_shape, weight, bias, eps)\u001B[0m\n\u001B[1;32m   2890\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight, bias):\n\u001B[1;32m   2891\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   2892\u001B[0m         layer_norm,\n\u001B[1;32m   2893\u001B[0m         (\u001B[38;5;28minput\u001B[39m, weight, bias),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2898\u001B[0m         eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m   2899\u001B[0m     )\n\u001B[0;32m-> 2900\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2901\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[1;32m   2902\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.plot(train_losses)",
   "id": "ff1b01ac0e48b754",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "plt.plot(test_losses)"
   ],
   "id": "c862ce9afbcc0d0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.plot(test_losses[-10:])",
   "id": "64e5772ff0383f50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_loss = 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # for inputs, labels in tqdm(test_loader, desc=f\"Testing Epoch {epoch_num}\"):\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(-1), labels)\n",
    "        # test_loss += loss.item()\n",
    "        val_loss += loss.item() * inputs.size(0)  # Multiply batch loss by batch size\n",
    "    avg_loss = val_loss/len(testing_dataset)\n",
    "    print(f\"ON VAL SET, ACHIEVED {avg_loss:.4f}\")"
   ],
   "id": "80fefc83a8dbb686",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "567e227e95c6bca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_dataset = torch.load(\"../data/2_ready_for_training/embedded/val_dataset_large.pth\")\n",
    "\n",
    "# Assuming val_dataset is a list of (inputs, labels)\n",
    "inputs_list = []\n",
    "labels_list = []\n",
    "\n",
    "for item in val_dataset:\n",
    "    inputs, labels = item  # Unpack inputs and labels\n",
    "    inputs_list.append(inputs.cpu().numpy())  # Convert tensors to numpy arrays\n",
    "    labels_list.append(labels.cpu().numpy())"
   ],
   "id": "a077bf1390ae9e00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "labels_list_flat = [int(i) for i in labels_list]",
   "id": "2150c197e87bea1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# inputs_list_flat = [inp.flatten() for inp in inputs_list]  # Flatten the inputs\n",
    "# labels_list_flat = [lbl.flatten() for lbl in labels_list]  # Flatten the labels\n",
    "\n",
    "val_df = pl.DataFrame({\n",
    "    # \"inputs\": inputs_list_flat,  # Polars will handle the flat numpy arrays\n",
    "    \"labels\": labels_list_flat\n",
    "})\n"
   ],
   "id": "26b8dd8b0a603060",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Assuming val_dataset, batch_size, and model are already defined\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_loss = 0.0\n",
    "model.eval()\n",
    "predictions = []  # List to store predictions in order\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs.squeeze(-1), labels)\n",
    "        val_loss += loss.item() * inputs.size(0)  # Multiply batch loss by batch size\n",
    "\n",
    "        # Collect predictions\n",
    "        predictions.extend(outputs.squeeze(-1).cpu().tolist())\n",
    "\n",
    "# Compute the average loss\n",
    "avg_loss = val_loss / len(val_dataset)\n",
    "print(f\"ON VAL SET, ACHIEVED {avg_loss:.4f}\")\n"
   ],
   "id": "d7f2480d69c26512",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use the predictions to create a Polars column\n",
    "val_df = val_df.with_columns(pl.Series(predictions).alias(\"large_embed_predictions\"))\n",
    "val_df.write_csv(\"../outputs/predictions/large_embed_predictions.csv\")"
   ],
   "id": "2318a0fe7ff16fd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"../models/embedding/nn_on_large_embeddings.pth\")",
   "id": "a7d0f7fbf0b9004c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
