{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# standard python imports\n",
    "import os\n",
    "\n",
    "base_model = \"../models/llama_base/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\"\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# huggingface libraries\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM # , setup_chat_format"
   ],
   "id": "417b29b2d52bc205",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"optim00\",\n",
    "    name=\"essay01\",\n",
    "    config={\n",
    "        \"model_name\": \"local_training_run_00\",\n",
    "        \"task\": \"response_only\",\n",
    "        \"timestamp\": \"2024.11.18.18_02\"\n",
    "    }\n",
    ")\n",
    "\n",
    "new_model = \"../models/llama_finetuned/\"\n",
    "\n",
    "PATH_data_to_train_on = \"../data/1_clean/training.csv\"\n",
    "PATH_data_to_test_on = \"../data/1_clean/testing.csv\""
   ],
   "id": "abc9c7c607370233",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # NOTE WAS 4 BIT\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Match input dtype\n",
    ")\n",
    "model = LlamaForCausalLM.from_pretrained(base_model, quantization_config=nf4_config, device_map=\"auto\")"
   ],
   "id": "1c27846c02e058a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print(torch.cuda.is_available())\n",
    "\n",
    "# with a bigger gpu could try this\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model,\n",
    "#     device_map=\"auto\",\n",
    "#     device_map=\"balanced\",\n",
    "    # torch_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    tokenizer_file=os.path.join(base_model, 'tokenizer.json'),\n",
    "    tokenizer_config_file=os.path.join(base_model, 'tokenizer_config.json'),\n",
    "    special_tokens_map_file=os.path.join(base_model, 'special_tokens_map.json'),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token_id = 128004  # tokenizer.convert_tokens_to_ids(\"<|finetune_right_pad_id|>\")\n",
    "model.config.pad_token_id = 128004  # tokenizer.convert_tokens_to_ids(\"<|finetune_right_pad_id|>\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "id": "73bf04e07ff45bbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_trainable_params(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_percentage = 100 * trainable_params / total_params\n",
    "    print(f\"{trainable_percentage:,.2f}% of parameters are trainable\")\n",
    "    print(f\"{trainable_params:,.2f} many parameters are trainable\")\n",
    "\n",
    "print_trainable_params(model)\n",
    "\n",
    "def create_prompt(review):\n",
    "    system_prompt = f\"You read student essays reviews and return a score from 0 to 60 that represents your besst guess of the number of rating given by the grader. Return just the number 0, 1, ..., 60 with no context, explanation, or special symbols.\"\n",
    "    prompt = f\"Here is the review to evaluate: [[[{review}]]]. You read student essays reviews and return a score from 0 to 60 that represents your besst guess of the number of rating given by the grader. Return just the number 0, 1, ..., 60 with no context, explanation, or special symbols.\"\n",
    "\n",
    "    return system_prompt, prompt\n",
    "\n",
    "df_train = pl.read_csv(PATH_data_to_train_on)\n",
    "df_test = pl.read_csv(PATH_data_to_test_on)\n",
    "\n",
    "print(f\"{df_train.shape=}\")\n",
    "print(f\"{df_test.shape=}\")"
   ],
   "id": "2610d2e7ea424a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lst_system_prompt, lst_prompt = [], []\n",
    "for row in df_train.iter_rows(named=True):\n",
    "    system_prompt, prompt = create_prompt(row[\"text\"])\n",
    "    lst_system_prompt.append(system_prompt)\n",
    "    lst_prompt.append(prompt)\n",
    "df_train = df_train.with_columns(pl.Series(lst_system_prompt).alias(\"instruction\"),\n",
    "                                 pl.Series(lst_prompt).alias(\"input\"))\n",
    "output = [int(i) for i in df_train[\"score\"].to_list()]\n",
    "df_train = df_train.with_columns(pl.Series(output).alias(\"output\"))\n",
    "\n",
    "lst_system_prompt, lst_prompt = [], []\n",
    "for row in df_test.iter_rows(named=True):\n",
    "    system_prompt, prompt = create_prompt(row[\"text\"])\n",
    "    lst_system_prompt.append(system_prompt)\n",
    "    lst_prompt.append(prompt)\n",
    "df_test = df_test.with_columns(pl.Series(lst_system_prompt).alias(\"instruction\"),\n",
    "                               pl.Series(lst_prompt).alias(\"input\"))\n",
    "output = [int(i) for i in df_test[\"score\"].to_list()]\n",
    "df_test = df_test.with_columns(pl.Series(output).alias(\"output\"))\n",
    "\n",
    "train_dataset = Dataset.from_polars(df_train)\n",
    "test_dataset = Dataset.from_polars(df_test)"
   ],
   "id": "83adcd6fd1af6872",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def TO_GET_LEN(tokenizer):\n",
    "#     def TO_GET_LEN_INNER(row):\n",
    "#         row_json = [{\"role\": \"system\", \"content\": row[\"instruction\"]},\n",
    "#                     {\"role\": \"user\", \"content\": row[\"input\"]},\n",
    "#                     {\"role\": \"assistant\", \"content\": row[\"output\"]}]\n",
    "#\n",
    "#         row[\"list_of_tokens\"] = tokenizer.apply_chat_template(row_json, tokenize=True)\n",
    "#         return row\n",
    "#\n",
    "#     return TO_GET_LEN_INNER\n",
    "#\n",
    "# tmp_dataset1 = train_dataset.map(\n",
    "#     TO_GET_LEN(tokenizer),\n",
    "# )\n",
    "#\n",
    "# tmp_dataset2 = test_dataset.map(\n",
    "#     TO_GET_LEN(tokenizer),\n",
    "# )\n",
    "#\n",
    "# max_seq_length_needed1 = max(tmp_dataset1.map(lambda x: {\"length\": len(x[\"list_of_tokens\"])})[\"length\"]) + 1\n",
    "# max_seq_length_needed2 = max(tmp_dataset2.map(lambda x: {\"length\": len(x[\"list_of_tokens\"])})[\"length\"]) + 1\n",
    "# max_seq_length_needed=max(max_seq_length_needed1, max_seq_length_needed2)\n",
    "# print(f\"{max_seq_length_needed:,.2f}\")"
   ],
   "id": "ec272ba9c2ac2d6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this is calculated above, hardcoding since it is constant run to run and saves compute\n",
    "max_seq_length_needed = 1_631"
   ],
   "id": "65740a2967fa19f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def format_but_not_tokenize(example):\n",
    "    test = example[\"instruction\"]\n",
    "    # assert isinstance(test, list), \"Input 'example' must be a list, this is probably because formatting function needs >1 eg\"\n",
    "    # assert not isinstance(test, str), \"Input 'example' must be a list, not a string\"\n",
    "\n",
    "    output_texts = []\n",
    "\n",
    "    if isinstance(test, list):\n",
    "        K_range = len(test)\n",
    "\n",
    "        for i in range(K_range):\n",
    "            row_json = [{\"role\": \"system\", \"content\": example['instruction'][i]},\n",
    "                        {\"role\": \"user\", \"content\": example['input'][i]},\n",
    "                        {\"role\": \"assistant\", \"content\": example['output'][i]}]\n",
    "            text = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "            output_texts.append(text)\n",
    "\n",
    "    elif isinstance(test, str):\n",
    "        # K_range = 1\n",
    "        row_json = [{\"role\": \"system\", \"content\": example['instruction']},\n",
    "                    {\"role\": \"user\", \"content\": example['input']},\n",
    "                    {\"role\": \"assistant\", \"content\": example['output']}]\n",
    "        text = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "        output_texts.append(text)\n",
    "    else:\n",
    "        assert False, \"ERROR: WHAT IS GOING INTO FORMAT_BUT_NOT_TOKENIZE???\"\n",
    "\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ],
   "id": "4df48e06b780e351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False  # Disable KV cache during training"
   ],
   "id": "d875f05b9335ac49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# training_args = SFTConfig(\n",
    "#     max_seq_length=max_seq_length_needed,\n",
    "#     output_dir=new_model,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     gradient_accumulation_steps=2,  # 4\n",
    "#     # optim=\"adamw_torch\",\n",
    "#     # optim=\"paged_adamw_32bit\",\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     num_train_epochs=5,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=0.2,\n",
    "#     logging_steps=10,\n",
    "#     warmup_steps=500,\n",
    "#     logging_strategy=\"steps\",\n",
    "#     learning_rate=2e-4,\n",
    "#     fp16=False,\n",
    "#     bf16=True,  # was false\n",
    "#     group_by_length=True,\n",
    "#     gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "#     report_to=\"wandb\",\n",
    "#     run_name=\"ESSAY00\"\n",
    "# )\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    max_seq_length=max_seq_length_needed,\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    # optim=\"paged_adamw_8bit\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,  # Evaluate every 500 steps\n",
    "    logging_steps=10,\n",
    "    warmup_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"ESSAY00\"\n",
    ")\n"
   ],
   "id": "7807a99911d06ae8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=format_but_not_tokenize,\n",
    "    data_collator=collator,\n",
    ")"
   ],
   "id": "1549c69ab2821de3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "596cbc81468d286",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!ls models/llama_finetuned",
   "id": "fcb314f95b904c42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# this obviously would take too long on consumer grade hardware so I ran it on the HPC, see `../z_for_running_on_hpc` for details",
   "id": "969fb04e167d3316"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
